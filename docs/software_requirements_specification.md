# BettaFish 软件需求规格说明书 (PRD) 框架大纲

## 1. 项目引言
### 1.1 项目背景与愿景
### 1.2 项目目标与价值主张
### 1.3 系统范围与边界定义
### 1.4 目标用户群体分析

## 2. 系统架构
### 2.1 整体技术架构
#### 2.1.1 微服务架构设计
#### 2.1.2 容器化部署方案
#### 2.1.3 多智能体协作机制
### 2.2 技术栈选型
#### 2.2.1 后端技术栈 (Python/Flask)
#### 2.2.2 前端技术栈 (Streamlit)
#### 2.2.3 数据库技术栈 (PostgreSQL)
#### 2.2.4 AI模型集成方案
### 2.3 数据流向架构
#### 2.3.1 数据采集与处理流程
#### 2.3.2 智能体间数据交换机制
#### 2.3.3 报告生成与输出流程

## 3. 核心功能需求
### 3.1 MindSpider 数据采集引擎

#### 3.1.1 整体架构与工作流程

**功能描述**：MindSpider是BettaFish系统的数据采集核心引擎，采用双模式并行架构，实现广度话题发现与深度情感分析的有机结合。

**工作流程图**：
```
用户输入 → MindSpider主程序 → 双模式并行处理
                        ↓
        广度话题提取(BroadTopicExtraction) ←→ 深度情感爬取(DeepSentimentCrawling)
                        ↓
                数据清洗与存储 → 数据库持久化
```

**关键逻辑**：
- **双模式协同**：广度模式负责全网热点发现，深度模式针对特定关键词进行精细化采集
- **异步处理**：采用异步IO模型，支持高并发数据采集
- **智能调度**：根据数据源特性自动调整采集策略和频率

#### 3.1.2 BroadTopicExtraction 广度话题提取

**功能描述**：通过聚合多个主流平台的热点榜单，快速发现全网热点话题，为后续深度分析提供目标方向。

**输入/输出**：
- **输入**：无特定输入（自动获取当日热点）
- **输出**：结构化热点话题列表，包含关键词、热度值、来源平台等信息

**关键逻辑**：
- **多源聚合**：集成微博热搜、知乎热榜、B站热搜、抖音热榜等12个主流平台
- **API调用**：通过统一新闻API接口获取标准化数据格式
- **智能分析**：使用DeepSeek模型自动提取关键词和生成新闻总结
- **数据存储**：将提取结果存入`daily_news`和`daily_topics`数据库表

**支持平台**：
| 平台代码 | 平台名称 | 数据内容 |
|---------|---------|---------|
| weibo | 微博热搜 | 热搜话题、热度值 |
| zhihu | 知乎热榜 | 热门问题、回答数 |
| bilibili-hot-search | B站热搜 | 热门视频、播放量 |
| douyin | 抖音热榜 | 热门视频、点赞数 |
| toutiao | 今日头条 | 热点新闻、阅读量 |
| github-trending-today | GitHub趋势 | 热门项目、star数 |
| tieba | 百度贴吧 | 热门帖子、回复数 |
| wallstreetcn | 华尔街见闻 | 财经新闻、关注度 |
| thepaper | 澎湃新闻 | 时事新闻、评论数 |
| cls-hot | 财联社 | 财经快讯、影响力 |
| xueqiu | 雪球热榜 | 投资话题、讨论数 |
| coolapk | 酷安热榜 | 应用推荐、下载量 |

#### 3.1.3 DeepSentimentCrawling 深度情感爬取

**功能描述**：针对特定关键词进行全平台深度数据采集，获取详细的内容信息和用户评论，支持情感分析。

**输入/输出**：
- **输入**：用户指定的关键词或话题
- **输出**：结构化内容数据、用户评论、创作者信息等

**关键逻辑**：
- **多平台覆盖**：支持7个主流社交媒体平台的深度爬取
- **浏览器自动化**：基于Playwright实现真实浏览器行为模拟
- **反爬虫机制**：支持代理IP轮换、User-Agent伪装、请求频率控制
- **数据完整性**：确保采集内容、评论、用户信息的完整关联

#### 3.1.4 多平台媒体数据采集

##### 3.1.4.1 微博数据采集
**采集字段**：
- **内容信息**：微博ID、内容文本、发布时间、点赞数、评论数、转发数
- **用户信息**：用户ID、昵称、头像、性别、IP属地、粉丝数、关注数
- **评论信息**：评论ID、评论内容、点赞数、回复数、评论时间

##### 3.1.4.2 抖音数据采集  
**采集字段**：
- **视频信息**：视频ID、标题、描述、点赞数、评论数、分享数、收藏数
- **用户信息**：用户ID、昵称、头像、签名、IP属地、粉丝数
- **评论信息**：评论ID、评论内容、点赞数、图片信息、回复关系

##### 3.1.4.3 小红书数据采集
**采集字段**：
- **笔记信息**：笔记ID、标题、描述、点赞数、收藏数、评论数、分享数
- **用户信息**：用户ID、昵称、头像、IP属地、粉丝数、关注数、标签
- **评论信息**：评论ID、评论内容、点赞数、回复关系

##### 3.1.4.4 B站数据采集
**采集字段**：
- **视频信息**：视频ID、标题、描述、播放量、弹幕数、点赞数、投币数
- **用户信息**：用户ID、昵称、头像、签名、粉丝数、获赞数
- **评论信息**：评论ID、评论内容、点赞数、回复数、评论时间

##### 3.1.4.5 知乎数据采集
**采集字段**：
- **问题信息**：问题ID、标题、描述、回答数、关注数、浏览数
- **回答信息**：回答ID、内容、点赞数、评论数、创建时间
- **用户信息**：用户ID、昵称、头像、签名、粉丝数

##### 3.1.4.6 快手数据采集
**采集字段**：
- **视频信息**：视频ID、标题、描述、点赞数、观看数、分享数
- **用户信息**：用户ID、昵称、头像、IP属地
- **评论信息**：评论ID、评论内容、回复数、评论时间

##### 3.1.4.7 贴吧数据采集
**采集字段**：
- **帖子信息**：帖子ID、标题、内容、回复数、查看数、发布时间
- **用户信息**：用户ID、昵称、头像、等级
- **回复信息**：回复ID、内容、点赞数、回复时间

#### 3.1.5 数据清洗与存储机制

**功能描述**：对采集的原始数据进行清洗、去重、格式化处理，并持久化存储到数据库。

**关键逻辑**：
- **数据清洗**：去除HTML标签、表情符号、特殊字符，统一编码格式
- **数据去重**：基于内容哈希值进行重复内容检测和过滤
- **格式标准化**：统一时间格式、数字格式、文本编码
- **关联存储**：确保内容、评论、用户信息的关联关系完整性

**存储架构**：
- **数据库支持**：MySQL、PostgreSQL、SQLite多数据库支持
- **表结构设计**：按平台分表存储，包含内容表、评论表、用户表
- **索引优化**：对关键字段建立索引，提升查询性能
- **数据备份**：支持定时备份和增量备份机制

**数据流处理**：
```
原始数据 → 数据清洗 → 格式标准化 → 去重处理 → 关联存储 → 数据库
```

### 3.2 QueryEngine 精准搜索引擎

#### 3.2.1 核心功能定义与系统角色

**功能描述**：QueryEngine是BettaFish系统的情报搜集核心引擎，负责将用户的自然语言查询转化为精准的搜索策略，通过多轮反思迭代机制，实现深度信息挖掘和高质量内容生成。

**系统角色**：
- **情报搜集器**：从公开网络获取最新、最相关的信息
- **信息过滤器**：通过智能算法去除噪音和重复内容
- **内容生成器**：基于搜索结果生成结构化的深度分析报告
- **质量控制器**：通过反思机制确保信息的准确性和完整性

**国内外信息处理策略**：
- **国内信息**：优先使用中文关键词，关注国内主流媒体和社交媒体
- **国际信息**：支持英文搜索，整合国际权威媒体和学术资源
- **多语言支持**：自动识别查询语言并适配相应的搜索策略

#### 3.2.2 底层运行机制与工作流图谱

**工作流图谱**：
```
用户查询 → 报告结构生成 → 段落并行处理 → 反思迭代 → 报告格式化
                        ↓
        首次搜索 → 首次总结 → 反思搜索 → 反思总结 → 段落完成
```

**关键逻辑**：
- **非线性执行流程**：采用反思迭代机制，支持多轮信息补充和优化
- **段落并行处理**：多个段落可以同时进行搜索和总结，提升效率
- **状态驱动**：基于StateGraph的状态管理，确保数据一致性

**反思循环机制**：
```
首次搜索 → 首次总结 → 反思检查 → [需要补充] → 反思搜索 → 反思总结
                        ↓
                  [内容完整] → 段落完成
```

#### 3.2.3 搜索策略与查询优化

**自然语言转搜索关键词**：
- **语义理解**：使用LLM分析用户查询的深层意图和关键词
- **工具选择**：根据查询类型自动选择最合适的搜索工具
- **时间范围**：智能识别是否需要时间限制（24小时、一周、自定义）
- **搜索深度**：评估信息需求深度，选择基础搜索或深度分析

**搜索工具智能选择逻辑**：
| 查询特征 | 推荐工具 | 选择理由 |
|---------|---------|---------|
| 一般性查询 | basic_search_news | 快速、通用，适合大多数场景 |
| 深度分析需求 | deep_search_news | 提供详细分析和高级摘要 |
| 最新动态 | search_news_last_24_hours | 获取24小时内最新信息 |
| 近期趋势 | search_news_last_week | 分析一周内发展脉络 |
| 可视化需求 | search_images_for_news | 获取相关图片和视觉资料 |
| 历史研究 | search_news_by_date | 指定时间范围进行精确搜索 |

#### 3.2.4 去重与数据清洗机制

**重复内容检测**：
- **URL去重**：基于URL哈希值识别重复来源
- **内容相似度**：使用文本相似度算法检测重复内容
- **时间窗口**：同一来源在特定时间窗口内只保留最新内容

**数据清洗流程**：
```
原始搜索结果 → HTML标签清理 → 特殊字符处理 → 编码统一 → 格式标准化
```

**噪音过滤策略**：
- **广告内容识别**：基于关键词和模式识别广告内容
- **低质量源过滤**：过滤权威性低的新闻源
- **重复信息合并**：合并相同事件的不同报道

#### 3.2.5 工具与外部API集成

**核心搜索工具集**：
- **Tavily News Agency**：主要搜索工具，提供6种专业搜索功能
- **支持功能**：基础搜索、深度分析、时间范围搜索、图片搜索

**API配置参数**：
```python
# Tavily API配置
TAVILY_API_KEY = "your_api_key"
search_depth = "basic" | "advanced"  # 搜索深度
max_results = 5-20                    # 结果数量
time_range = "d" | "w" | "custom"     # 时间范围
include_answer = False | "advanced"   # 是否包含AI摘要
```

**重试与容错机制**：
- **指数退避重试**：网络错误时自动重试，最多3次
- **优雅降级**：API不可用时提供友好的错误信息
- **超时控制**：设置合理的超时时间，避免长时间等待

#### 3.2.6 状态管理与数据结构

**QueryState核心数据结构**：
```python
@dataclass
class State:
    query: str                    # 原始用户查询
    report_title: str             # 报告标题
    paragraphs: List[Paragraph]  # 段落列表
    final_report: str            # 最终报告内容
    is_completed: bool           # 是否完成
    created_at: str              # 创建时间
    updated_at: str              # 更新时间
```

**段落状态管理**：
```python
@dataclass
class Paragraph:
    title: str                    # 段落标题
    content: str                  # 预期内容
    research: Research           # 研究进度
    order: int                   # 段落顺序
```

**研究过程状态**：
```python
@dataclass
class Research:
    search_history: List[Search]  # 搜索记录
    latest_summary: str          # 最新总结
    reflection_iteration: int    # 反思迭代次数
    is_completed: bool          # 是否完成
```

**关键状态字段说明**：
- **search_results**：存储所有搜索结果的详细信息
- **summary**：每个段落的当前总结状态
- **news_list**：经过清洗和去重后的新闻列表
- **reflection_count**：记录反思迭代次数，控制循环深度

#### 3.2.7 提示词模板与JSON Schema

**结构化提示词设计**：
- **系统角色定义**：明确AI助手的专业角色和任务目标
- **JSON Schema约束**：使用严格的JSON格式确保输出一致性
- **工具说明集成**：在提示词中嵌入工具使用说明

**核心提示词类别**：
1. **报告结构生成**：SYSTEM_PROMPT_REPORT_STRUCTURE
2. **首次搜索**：SYSTEM_PROMPT_FIRST_SEARCH  
3. **首次总结**：SYSTEM_PROMPT_FIRST_SUMMARY
4. **反思搜索**：SYSTEM_PROMPT_REFLECTION
5. **反思总结**：SYSTEM_PROMPT_REFLECTION_SUMMARY
6. **报告格式化**：SYSTEM_PROMPT_REPORT_FORMATTING

**JSON Schema规范**：
- **输入验证**：确保输入数据符合预期格式
- **输出约束**：强制输出符合指定的JSON结构
- **错误处理**：提供清晰的错误提示和修复建议

#### 3.2.8 性能优化与扩展性

**性能优化策略**：
- **异步处理**：支持并发搜索和并行段落处理
- **缓存机制**：对重复查询结果进行缓存
- **增量更新**：支持对已有报告的增量式更新

**扩展性设计**：
- **插件化架构**：支持新的搜索工具快速集成
- **多LLM支持**：可配置不同的LLM模型
- **自定义提示词**：支持用户自定义提示词模板

**监控与日志**：
- **执行追踪**：记录每个节点的执行状态和耗时
- **错误日志**：详细的错误信息和调试信息
- **性能指标**：搜索成功率、响应时间、内容质量等指标

### 3.3 MediaEngine 多模态分析引擎

#### 3.3.1 核心功能定义与系统角色

**功能描述**：MediaEngine是BettaFish系统的多模态分析核心引擎，专门负责处理视觉、图像和多媒体内容，通过强大的多模态搜索能力，实现图文、视频、结构化数据的深度整合分析。

**系统角色**：
- **视觉信息处理器**：解析图片、视频等非文本内容，提取关键视觉要素
- **多模态搜索器**：整合网页、图片、结构化数据等多种信息源
- **模态卡解析器**：专门处理天气、股票、百科等结构化信息卡片
- **多媒体内容生成器**：基于多模态信息生成深度分析报告

**补充纯文本分析的不足**：
- **视觉信息补充**：为纯文本分析提供图片、视频等视觉证据
- **情感维度增强**：通过视觉元素分析增强情感倾向性判断
- **数据完整性**：提供天气、股票等结构化数据支持
- **信息多样性**：整合文字、图像、数据等多种信息类型

#### 3.3.2 底层运行机制与多模态输入处理

**多模态输入处理机制**：
```
多模态查询 → Bocha/Anspire API → 多模态结果解析 → 信息分类处理
                        ↓
        网页内容 → 图片信息 → 模态卡数据 → AI总结 → 追问建议
```

**视觉信息传输方式**：
- **URL传输**：图片和视频内容通过URL链接传递给LLM
- **Base64编码**：支持小尺寸图片的Base64编码传输
- **元数据提取**：提取图片尺寸、格式、颜色空间等元信息
- **缩略图处理**：自动生成适合LLM处理的缩略图版本

**视频内容处理策略**：
- **关键帧抽取**：从视频中提取代表性关键帧进行分析
- **时间戳标记**：记录关键帧在视频中的时间位置
- **音频分离**：支持音频内容的文本转录和分析
- **动态分析**：分析视频中的运动、节奏、场景变化

#### 3.3.3 平台适配逻辑与视觉要素提取

**平台差异化分析策略**：
| 平台类型 | 分析重点 | 特色功能 |
|---------|---------|---------|
| **抖音/快手** | 短视频内容、音乐情感、用户互动 | 视频节奏分析、背景音乐情感识别 |
| **小红书** | 图文笔记、生活方式、产品评测 | 图片美学分析、生活方式标签提取 |
| **微博** | 热点话题、图片新闻、用户评论 | 话题传播分析、图片新闻解读 |
| **B站** | 长视频、弹幕文化、知识分享 | 视频内容深度分析、弹幕情感识别 |

**视觉要素提取要求**：
- **画面色调分析**：提取主色调、色彩饱和度、明暗对比
- **人物表情识别**：分析面部表情、肢体语言、情感状态
- **文字OCR提取**：识别图片中的文字内容，支持多语言
- **背景音乐情感**：分析音乐节奏、音调、情感倾向
- **构图美学评估**：分析画面构图、对称性、视觉焦点
- **品牌标识识别**：检测图片中的品牌Logo和标识

#### 3.3.4 LLM交互逻辑与Vision提示词构建

**Vision提示词构建策略**：
```python
# 多模态提示词结构
prompt_structure = {
    "system_role": "专业的多媒体内容分析师",
    "task_description": "基于多模态信息进行深度分析",
    "visual_analysis_requirements": [
        "画面色调和色彩分析",
        "人物表情和情感识别", 
        "文字内容和OCR提取",
        "构图美学评估",
        "品牌标识检测"
    ],
    "integration_requirements": "文字、图像、数据的有机结合"
}
```

**Token限制处理策略**：
- **内容分块**：将大型图片或视频分割为多个分析单元
- **优先级排序**：根据重要性对视觉元素进行优先级排序
- **摘要提取**：对非关键视觉信息进行摘要处理
- **渐进式分析**：支持多轮渐进式的深度分析

#### 3.3.5 多模态搜索工具与API集成

**核心搜索工具集**：
- **Bocha Multimodal Search**：主要多模态搜索工具，支持5种专业功能
- **Anspire AI Search**：备选搜索工具，提供网页搜索能力

**多模态搜索功能**：
| 工具名称 | 功能描述 | 返回内容 |
|---------|---------|---------|
| **comprehensive_search** | 全面综合搜索 | 网页、图片、AI总结、追问建议、模态卡 |
| **web_search_only** | 纯网页搜索 | 网页链接和摘要，速度快成本低 |
| **search_for_structured_data** | 结构化数据查询 | 天气、股票、汇率、百科等模态卡 |
| **search_last_24_hours** | 24小时内信息 | 最新动态和突发事件 |
| **search_last_week** | 本周信息搜索 | 近期发展趋势和报道 |

**模态卡支持类型**：
- **天气信息**：weather_china - 中国天气数据
- **股票数据**：stock - 股票行情和走势
- **汇率信息**：exchange_rate - 货币汇率数据
- **百科知识**：baike_pro - 百科词条解释
- **医疗信息**：medical_common - 常见医疗知识
- **交通信息**：train_ticket - 火车票信息
- **汽车参数**：car_parameter - 汽车技术参数

#### 3.3.6 数据结构与信息整合

**多模态数据结构**：
```python
@dataclass
class BochaResponse:
    query: str                    # 搜索查询
    conversation_id: str         # 会话ID
    answer: str                  # AI生成的总结答案
    follow_ups: List[str]        # AI生成的追问建议
    webpages: List[WebpageResult] # 网页搜索结果
    images: List[ImageResult]     # 图片搜索结果
    modal_cards: List[ModalCardResult] # 模态卡结构化数据
```

**视觉信息数据结构**：
```python
@dataclass
class ImageResult:
    name: str                    # 图片名称
    content_url: str             # 图片内容URL
    host_page_url: str           # 来源页面URL
    thumbnail_url: str           # 缩略图URL
    width: int                   # 图片宽度
    height: int                  # 图片高度
```

**模态卡数据结构**：
```python
@dataclass
class ModalCardResult:
    card_type: str               # 卡片类型
    content: Dict[str, Any]      # 解析后的JSON内容
```

#### 3.3.7 信息整合与内容生成

**多模态信息整合策略**：
- **关联性分析**：分析不同信息源之间的内在联系
- **一致性验证**：验证文字描述与视觉内容的一致性
- **互补性利用**：充分利用不同信息源的互补优势
- **冲突处理**：处理不同信息源之间的冲突和矛盾

**内容生成标准**：
- **信息密度要求**：每100字包含2-3个多模态信息点
- **视觉化描述**：用文字生动描述图片和视觉内容
- **数据可视化**：将数字信息转化为易理解的描述
- **立体化分析**：从多个感官维度进行综合分析

#### 3.3.8 性能优化与扩展性

**性能优化策略**：
- **并行处理**：支持多个模态信息的并行分析
- **缓存机制**：对重复的视觉分析结果进行缓存
- **增量更新**：支持对已有分析的增量式更新
- **负载均衡**：根据资源情况动态调整分析深度

**扩展性设计**：
- **插件化架构**：支持新的视觉分析算法快速集成
- **多模型支持**：可配置不同的Vision模型
- **自定义分析**：支持用户自定义视觉分析要求
- **格式兼容**：支持多种图片和视频格式

**质量监控**：
- **分析准确性**：监控视觉分析的准确性和一致性
- **处理效率**：跟踪不同规模内容的处理效率
- **资源消耗**：监控计算资源和存储资源的使用情况
- **用户反馈**：收集用户对分析结果的满意度反馈

### 3.4 InsightEngine 智能洞察引擎

#### 3.4.1 核心功能定义与数据关系

**功能描述**：InsightEngine是BettaFish系统的智能洞察核心引擎，专门负责深度挖掘`bettafish`数据库中的海量评论数据，通过多维度分析提供深度洞察。

**与MindSpider的数据关系**：
```
MindSpider数据采集 → bettafish数据库存储 → InsightEngine深度挖掘 → 智能洞察输出
```

**数据流关系**：
- **数据来源**：MindSpider采集的7大社交媒体平台数据（B站、抖音、微博、小红书、快手、知乎、贴吧）
- **数据规模**：海量评论、视频、文章、用户互动数据
- **数据时效性**：支持24小时、一周、一年等不同时间范围的数据分析
- **数据完整性**：包含内容、评论、标签、互动指标等完整信息

**核心洞察能力**：
- **情感洞察**：基于多语言情感分析模型的情感倾向性分析
- **热度洞察**：基于加权算法的综合热度计算和趋势分析
- **话题洞察**：基于关键词优化的精准话题挖掘
- **用户洞察**：基于互动数据的用户行为分析

#### 3.4.2 SQL生成与执行机制

**SQL生成策略**：
- **参数化查询**：所有SQL查询使用参数化方式，避免SQL注入
- **动态表结构适配**：通过`SHOW COLUMNS`自动适配不同平台的数据表结构
- **统一字段映射**：将不同平台的字段名映射为统一的互动指标

**SQL执行流程**：
```python
# 参数化查询示例
def search_topic_globally(self, topic: str, limit_per_table: int = 100):
    search_term = f"%{topic}%"
    param_dict = {}
    where_clauses = []
    
    # 动态构建WHERE条件
    for idx, field in enumerate(config['fields']):
        pname = f"term_{idx}"
        where_clauses.append(f'{self._wrap_query_field_with_dialect(field)} LIKE :{pname}')
        param_dict[pname] = search_term
    
    param_dict['limit'] = limit_per_table
    where_clause = " OR ".join(where_clauses)
    
    # 安全的参数化查询
    query = f'SELECT * FROM {self._wrap_query_field_with_dialect(table)} WHERE {where_clause} ORDER BY id DESC LIMIT :limit'
    raw_results = self._execute_query(query, param_dict)
```

**数据库方言支持**：
- **MySQL**：使用`aiomysql`异步驱动
- **PostgreSQL**：使用`asyncpg`异步驱动
- **自动适配**：根据配置自动选择数据库方言

#### 3.4.3 关键词优化器工作原理

**优化目标**：将Agent生成的搜索词优化为更适合舆情数据库查询的关键词

**优化策略**：
```python
class KeywordOptimizer:
    """关键词优化器 - 使用Qwen3模型优化搜索词"""
    
    def optimize_keywords(self, original_query: str, context: str = "") -> KeywordOptimizationResponse:
        """
        优化原则：
        1. 贴近网民语言：使用普通网友在社交媒体上会使用的词汇
        2. 避免专业术语：不使用"舆情"、"传播"、"倾向"等官方词汇
        3. 简洁具体：每个关键词要非常简洁明了，便于数据库匹配
        4. 情感丰富：包含网民常用的情感表达词汇
        5. 数量控制：最少10个关键词，最多20个关键词
        """
```

**优化示例**：
```
输入："武汉大学舆情管理 未来展望 发展趋势"
输出：["武大", "武汉大学", "学校管理", "武大教育"]
理由：选择'武大'和'武汉大学'作为核心词汇，这是网民最常使用的称呼；
      '学校管理'比'舆情管理'更贴近日常表达；
      避免使用'未来展望'、'发展趋势'等网民很少使用的专业术语
```

**优化流程**：
1. **Qwen3模型调用**：使用硅基流动的Qwen3模型进行智能优化
2. **JSON格式解析**：解析模型返回的JSON格式关键词列表
3. **关键词验证**：验证关键词质量，过滤不良关键词
4. **备用方案**：当API调用失败时，使用基于规则的关键词提取

#### 3.4.4 情感计算流程与模型架构

**情感分析流程**：
```
文本预处理 → 多语言模型推理 → 情感标签映射 → 置信度计算 → 结果输出
```

**支持的情感模型架构**：

**1. 多语言情感分析模型（主模型）**
- **模型名称**：tabularisai/multilingual-sentiment-analysis
- **支持语言**：中文、英文、西班牙文、阿拉伯文、日文、韩文等22种语言
- **情感等级**：5级分类（非常负面、负面、中性、正面、非常正面）
- **技术架构**：基于Transformers的预训练模型

**2. BERT情感分析模型**
- **模型架构**：BERT + 分类器网络
- **输入处理**：中文BERT预训练模型（bert-base-chinese）
- **分类方式**：二分类（正面/负面）或五分类
- **训练数据**：微博情感分析语料库

**3. LSTM情感分析模型**
- **模型架构**：双向LSTM网络
- **词向量**：Word2Vec词向量表示
- **特征提取**：LSTM捕捉序列依赖关系
- **适用场景**：长文本情感分析

**4. Qwen3-LoRA微调模型**
- **模型规模**：支持0.6B、4B、8B三种规模
- **微调技术**：LoRA（Low-Rank Adaptation）参数高效微调
- **优势**：结合大语言模型的语义理解能力
- **部署方式**：支持本地部署和云端推理

**情感计算执行方式**：
- **内存计算**：数据从数据库拉取到内存后调用PyTorch/TensorFlow模型计算
- **批量处理**：支持批量文本的情感分析，提高处理效率
- **GPU加速**：自动检测GPU设备，优先使用CUDA/MPS进行推理
- **设备适配**：支持CPU、GPU、Apple MPS等多种计算设备

#### 3.4.5 热度计算与加权算法

**热度计算权重定义**：
```python
# 互动指标权重配置
W_LIKE = 1.0      # 点赞权重
W_COMMENT = 5.0   # 评论权重（高价值互动）
W_SHARE = 10.0    # 分享/转发/收藏/投币权重（最高价值）
W_VIEW = 0.1      # 观看权重
W_DANMAKU = 0.5   # 弹幕权重
```

**平台差异化热度公式**：
```sql
-- B站视频热度计算
(COALESCE(liked_count, 0) * 1.0 + 
 COALESCE(video_comment, 0) * 5.0 + 
 COALESCE(video_share_count, 0) * 10.0 + 
 COALESCE(video_favorite_count, 0) * 10.0 + 
 COALESCE(video_coin_count, 0) * 10.0 + 
 COALESCE(video_danmaku, 0) * 0.5 + 
 COALESCE(video_play_count, 0) * 0.1)

-- 微博笔记热度计算  
(COALESCE(liked_count, 0) * 1.0 + 
 COALESCE(comments_count, 0) * 5.0 + 
 COALESCE(shared_count, 0) * 10.0)
```

**热度计算特点**：
- **平台适配**：不同平台使用不同的字段映射和计算公式
- **权重差异化**：根据互动价值设置不同的权重系数
- **数据归一化**：处理空值和异常值，确保计算稳定性
- **实时更新**：支持动态时间范围的热度计算

#### 3.4.6 数据安全与限制机制

**SQL注入防护**：
- **参数化查询**：所有用户输入都通过参数化方式传递
- **输入验证**：对日期格式、数字参数进行严格验证
- **表名安全**：使用反引号包装表名，防止SQL注入
- **权限控制**：数据库连接使用只读权限账户

**数据量限制机制**：
- **分页查询**：所有查询都使用LIMIT限制返回结果数量
- **平台限制**：每个平台表限制返回100条记录（可配置）
- **总量控制**：全局查询限制总返回记录数
- **内存保护**：大数据集查询时进行分批处理

**Token保护策略**：
- **结果采样**：对大量结果进行聚类采样，减少Token消耗
- **内容截断**：对过长文本进行智能截断处理
- **优先级排序**：根据重要性对结果进行优先级排序
- **渐进加载**：支持按需加载，避免一次性加载过多数据

**聚类采样算法**：
```python
def _cluster_and_sample_results(self, results: List, max_results: int = 50):
    """
    对搜索结果进行聚类并采样
    使用sentence-transformers进行文本聚类
    每个聚类返回代表性结果
    """
    if len(results) <= max_results:
        return results
    
    # 使用多语言MiniLM模型进行文本嵌入
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    embeddings = model.encode([r.title_or_content for r in results])
    
    # K-means聚类
    kmeans = KMeans(n_clusters=min(10, len(results)//5))
    clusters = kmeans.fit_predict(embeddings)
    
    # 从每个聚类中采样代表性结果
    sampled_results = []
    for cluster_id in set(clusters):
        cluster_indices = [i for i, c in enumerate(clusters) if c == cluster_id]
        # 每个聚类采样5个代表性结果
        sampled_indices = cluster_indices[:5] 
        sampled_results.extend([results[i] for i in sampled_indices])
    
    return sampled_results[:max_results]
```

#### 3.4.7 平台适配与数据整合

**支持的平台类型**：
| 平台 | 内容类型 | 主要字段 | 特色功能 |
|-----|---------|---------|---------|
| **B站** | 视频、评论 | title, desc, video_comment | 弹幕分析、投币收藏 |
| **抖音** | 短视频、评论 | title, desc, comment_count | 音乐情感、特效分析 |
| **微博** | 笔记、评论 | content, comments_count | 话题传播、转发分析 |
| **小红书** | 图文笔记、评论 | title, desc, tag_list | 生活方式标签、美学分析 |
| **快手** | 短视频、评论 | title, desc, viewd_count | 老铁文化、互动分析 |
| **知乎** | 文章、评论 | title, content_text, voteup_count | 知识分享、专业分析 |
| **贴吧** | 帖子、评论 | title, desc, total_replay_num | 社区文化、楼层分析 |

**数据整合策略**：
- **字段统一映射**：将不同平台的字段映射为统一的QueryResult格式
- **时间格式标准化**：支持时间戳、字符串、日期等多种时间格式
- **互动指标归一化**：统一likes、comments、shares等互动指标
- **内容类型分类**：自动识别video、note、content等不同类型

#### 3.4.8 性能优化与扩展性

**性能优化策略**：
- **异步数据库操作**：使用SQLAlchemy异步引擎提高并发性能
- **连接池管理**：配置连接池复用和连接回收机制
- **懒加载模型**：情感分析模型按需加载，减少内存占用
- **结果缓存**：对重复查询结果进行缓存，提高响应速度

**扩展性设计**：
- **插件化架构**：支持新的情感分析模型快速集成
- **多模型支持**：可配置使用不同的情感分析算法
- **自定义权重**：支持用户自定义热度计算权重
- **平台扩展**：新的社交媒体平台可快速接入

**质量监控**：
- **分析准确性**：监控情感分析的准确性和一致性
- **查询性能**：跟踪数据库查询的响应时间和资源消耗
- **模型效果**：评估不同情感分析模型的效果对比
- **用户反馈**：收集用户对洞察结果的满意度反馈

### 3.5 ReportEngine 智能报告生成引擎

#### 3.5.1 核心功能定义与"总编辑"角色

**功能描述**：ReportEngine是BettaFish系统的"总编辑"，负责将QueryEngine、MediaEngine、InsightEngine三大引擎产生的碎片化数据，转化为结构化、连贯的长篇深度报告。

**数据整合流程**：
```
QueryEngine → 精准搜索数据
MediaEngine → 多模态分析数据  → ReportEngine → 结构化深度报告
InsightEngine → 智能洞察数据
```

**"总编辑"核心职责**：
- **模板选择**：根据用户需求自动选择最合适的报告模板
- **篇幅规划**：动态计算每个章节的字数预算，防止报告头重脚轻
- **章节生成**：基于中间表示(IR)结构生成高质量章节内容
- **文档装订**：将零散章节拼装成完整文档，处理交叉引用
- **多格式渲染**：支持HTML交互式报告和PDF打印级报告

#### 3.5.2 中间表示(IR)数据结构与文档模型

**核心机制**：ReportEngine采用"先生成结构，后生成文本"的两阶段策略，通过中间表示(IR)实现内容与格式的分离。

**IR Schema架构**：
```python
# IR版本控制
IR_VERSION = "1.0"

# 支持的块类型（16种）
ALLOWED_BLOCK_TYPES = [
    "heading", "paragraph", "list", "table", "swotTable", "pestTable",
    "blockquote", "engineQuote", "hr", "code", "math", "figure", 
    "callout", "kpiGrid", "widget", "toc"
]

# 行内标记类型（11种）
ALLOWED_INLINE_MARKS = [
    "bold", "italic", "underline", "strike", "code", "link",
    "color", "font", "highlight", "subscript", "superscript"
]
```

**文档IR结构**：
```json
{
  "version": "1.0",
  "reportId": "uuid",
  "metadata": {
    "title": "报告标题",
    "subtitle": "副标题",
    "tocPlan": [],
    "hero": {},
    "themeTokens": {},
    "generatedAt": "2025-01-21T10:30:00Z"
  },
  "chapters": [
    {
      "chapterId": "S1",
      "title": "章节标题",
      "anchor": "section-1",
      "order": 10,
      "blocks": [
        {
          "type": "heading",
          "level": 2,
          "text": "二级标题",
          "anchor": "subsection-1"
        },
        {
          "type": "paragraph",
          "inlines": [
            {
              "text": "段落内容",
              "marks": [{"type": "bold"}]
            }
          ]
        }
      ]
    }
  ],
  "assets": {}
}
```

**IR设计优势**：
- **格式无关**：IR结构独立于最终输出格式（HTML/PDF）
- **语义丰富**：支持SWOT分析表、PEST分析表等专业文档元素
- **可扩展性**：通过Schema验证确保数据一致性
- **版本控制**：支持IR版本升级和向后兼容

#### 3.5.3 篇幅规划与预算机制

**WordBudgetNode工作原理**：
```python
class WordBudgetNode(BaseNode):
    """规划各章节字数与重点"""
    
    def run(self, sections, design, reports, forum_logs, query):
        """
        根据设计稿和所有素材规划章节字数，让LLM写作时有明确篇幅目标
        
        输入：
        - sections: 模板章节列表
        - design: 布局节点返回的设计稿
        - reports: 三引擎报告映射
        - forum_logs: 论坛日志原文
        - query: 用户查询词
        
        输出：
        {
            "totalWords": 10000,           # 总字数
            "globalGuidelines": ["准则1", "准则2"],  # 全局写作准则
            "chapters": [                  # 章节预算
                {
                    "chapterId": "S1",
                    "targetWords": 1500,   # 目标字数
                    "minWords": 1200,      # 最少字数
                    "maxWords": 1800,      # 最多字数
                    "keyPoints": ["重点1", "重点2"]
                }
            ]
        }
        """
```

**预算分配算法**：
1. **内容重要性评估**：根据章节在模板中的位置和重要性分配权重
2. **数据丰富度分析**：基于三引擎报告的数据量调整字数分配
3. **平衡性约束**：确保各章节字数分布合理，避免头重脚轻
4. **动态调整**：根据实际写作情况动态调整后续章节预算

**预算约束示例**：
```
报告总字数：10,000字
- 执行摘要：800-1,200字（8-12%）
- 背景分析：1,500-2,000字（15-20%）
- 核心发现：3,000-3,500字（30-35%）
- 深度分析：2,500-3,000字（25-30%）
- 结论建议：1,200-1,500字（12-15%）
```

#### 3.5.4 动态模板与布局系统

**DocumentLayoutNode功能**：
```python
class DocumentLayoutNode(BaseNode):
    """负责生成全局标题、目录与Hero设计"""
    
    def run(self, sections, template_markdown, reports, forum_logs, query):
        """
        综合模板+多源内容，生成全书的标题、目录结构与主题色板
        
        输出：
        {
            "title": "报告标题",
            "subtitle": "副标题",
            "tocPlan": [
                {
                    "chapterId": "S1",
                    "display": "第一章",
                    "description": "章节描述",
                    "anchor": "section-1"
                }
            ],
            "hero": {
                "title": "Hero标题",
                "subtitle": "Hero副标题",
                "kpiGrid": [
                    {"label": "数据点1", "value": "100%"},
                    {"label": "数据点2", "value": "85%"}
                ]
            },
            "themeTokens": {
                "primaryColor": "#2563eb",
                "secondaryColor": "#64748b"
            }
        }
        """
```

**模板解析机制**：
- **Markdown模板**：使用标准Markdown语法定义报告结构
- **章节切片**：将模板按标题层级切分为独立的章节单元
- **动态适配**：根据实际数据量动态调整章节深度和广度
- **主题继承**：支持主题色板在整篇报告中保持一致

**模板示例结构**：
```markdown
# 报告标题

## 执行摘要
- 背景概述
- 核心发现
- 主要结论

## 详细分析
### 数据概览
### 趋势分析
### 深度洞察

## 结论建议
```

#### 3.5.5 文档装订与章节整合

**DocumentComposer装订流程**：
```python
class DocumentComposer:
    """将章节拼接成Document IR的简单装订器"""
    
    def build_document(self, report_id, metadata, chapters):
        """
        把所有章节按order排序并注入唯一锚点，形成整本IR
        
        装订步骤：
        1. 按order排序章节
        2. 补充默认chapterId
        3. 防止anchor重复，生成全局唯一锚点
        4. 注入IR版本与生成时间戳
        5. 合并metadata/themeTokens/assets
        """
```

**锚点管理策略**：
- **唯一性保证**：使用`_seen_anchors`集合记录已使用的锚点
- **冲突解决**：重复锚点自动追加序号（如`section-1-2`）
- **目录映射**：从metadata.toc.customEntries构建chapterId到anchor的映射
- **交叉引用**：支持章节间的内部链接和跳转

**装订输出示例**：
```json
{
  "version": "1.0",
  "reportId": "report-20250121",
  "metadata": {
    "title": "舆情分析报告",
    "generatedAt": "2025-01-21T10:30:00Z",
    "toc": {
      "customEntries": [
        {"chapterId": "S1", "anchor": "executive-summary"}
      ]
    }
  },
  "chapters": [
    {
      "chapterId": "S1",
      "title": "执行摘要",
      "anchor": "executive-summary",
      "order": 10,
      "blocks": [...]
    }
  ]
}
```

#### 3.5.6 GraphRAG知识增强机制

**知识图谱构建流程**：
```python
class GraphBuilder:
    """基于结构化数据构建知识图谱"""
    
    def build(self, topic, states, forum_entries) -> Graph:
        """
        构建知识图谱，无需LLM进行实体/关系提取
        
        节点类型（5种）：
        - topic: 用户查询主题
        - engine: 四个引擎来源 (insight/media/query/host)
        - section: 报告段落/章节
        - search_query: 搜索关键词
        - source: 信息来源 URL
        
        关系类型（4种）：
        - analyzed_by: 主题由引擎分析 (Topic → Engine)
        - contains: 引擎包含段落 (Engine → Section)
        - searched: 段落执行搜索 (Section → SearchQuery)
        - found: 搜索发现来源 (SearchQuery → Source)
        """
```

**图谱查询增强**：
- **反向检索**：在写作阶段反向查询知识图谱，增加报告深度
- **关系挖掘**：发现数据间的隐含关联，提供更全面的分析视角
- **证据链构建**：建立从原始数据到分析结论的完整证据链
- **知识复用**：支持跨报告的知识积累和复用

**GraphRAG查询示例**：
```python
# 查询与"品牌声誉"相关的所有搜索记录
query = "品牌声誉"
results = graph_rag_query(
    query=query,
    graph=knowledge_graph,
    max_results=10
)
# 返回：相关搜索词、来源URL、分析结论等
```

#### 3.5.7 渲染与可视化技术栈

**HTML渲染器架构**：
```python
class HTMLRenderer:
    """Document IR → HTML 渲染器"""
    
    def render(self, document_ir):
        """
        渲染流程：
        1. 重置状态并串联 _render_head / _render_body
        2. 根据 themeTokens 构造 <head>，注入 CSS 变量
        3. 组装页面骨架（页眉/header、目录/toc、章节/blocks）
        4. 处理 Chart.js/词云组件，先校验与修复数据
        5. 输出末尾 JS，负责按钮交互与图表实例化
        """
```

**图表渲染技术**：
- **Chart.js集成**：支持折线图、柱状图、饼图、散点图等
- **数据验证**：内置ChartValidator+LLM兜底修复，防止非法配置
- **词云生成**：基于wordcloud2.js实现关键词可视化
- **数学公式**：集成MathJax支持LaTeX公式渲染
- **交互功能**：支持图表缩放、筛选、导出等交互操作

**PDF渲染技术栈**：
```python
class PDFRenderer:
    """基于WeasyPrint的PDF渲染器"""
    
    def __init__(self):
        """
        技术栈：
        - WeasyPrint: HTML转PDF核心引擎
        - Pango/Cairo: 字体渲染和图形处理
        - 思源宋体: 完美中文字体支持
        - SVG转换: 图表和公式转矢量图形
        """
```

**PDF优化特性**：
- **字体嵌入**：预置思源宋体子集的Base64字体，避免缺字
- **布局优化**：自动处理分页、页眉页脚、目录生成
- **矢量图形**：图表和公式转换为SVG，确保打印质量
- **跨平台兼容**：Windows/macOS/Linux全平台支持

#### 3.5.8 质量保障与错误处理

**JSON解析鲁棒性**：
```python
class RobustJSONParser:
    """鲁棒JSON解析器，支持多重修复策略"""
    
    def parse(self, raw, context_name, expected_keys):
        """
        修复策略：
        1. 清理markdown标记和思考内容
        2. 本地语法修复（括号平衡、逗号补全、控制字符转义）
        3. 使用json_repair库进行高级修复
        4. 可选的LLM辅助修复（最大3次尝试）
        """
```

**错误处理机制**：
- **章节生成重试**：内容稀疏时自动重试（最多3次）
- **结构验证**：IRValidator确保文档结构符合Schema
- **图表修复**：ChartReviewService自动修复损坏的图表配置
- **降级策略**：当高级功能失败时自动降级到基础功能

**性能优化策略**：
- **懒加载模型**：按需加载渲染器和验证器
- **缓存机制**：字体和库文件缓存，避免重复IO
- **异步处理**：支持图表生成和文件导出的异步操作
- **内存管理**：大数据集查询时的分批处理和内存保护

#### 3.5.9 模板库与扩展性

**内置报告模板**：
- **企业品牌声誉分析报告模板**：品牌监测和声誉管理
- **市场竞争格局舆情分析报告模板**：竞争情报分析
- **日常或定期舆情监测报告模板**：周期性监测报告
- **特定政策或行业动态舆情分析报告模板**：政策影响分析
- **社会公共热点事件分析报告模板**：突发事件分析
- **突发事件与危机公关舆情报告模板**：危机管理报告

**模板扩展机制**：
- **Markdown标准**：使用标准Markdown语法，易于创建新模板
- **变量替换**：支持模板中的动态变量替换
- **条件渲染**：根据数据可用性动态显示/隐藏章节
- **插件架构**：支持自定义渲染器和验证器的插件化集成

**自定义模板示例**：
```markdown
# {{report_title}}

## 执行摘要
{{executive_summary}}

## 数据概览
{{#if has_charts}}
### 关键指标
{{kpi_chart}}
{{/if}}

## 详细分析
{{analysis_content}}
```

### 3.6 ForumEngine 智能体论坛协作机制

#### 3.6.1 核心功能定义与"虚拟会议室"架构

**功能描述**：ForumEngine是BettaFish系统的"虚拟会议室"，打破传统"线性执行"的局限，建立一个让QueryEngine、MediaEngine、InsightEngine三大智能体进行非线性辩论、交叉验证和思维融合的协作平台。

**系统角色定位**：
- **集体大脑**：将三个专业智能体的分析能力整合为集体智能
- **冲突调解器**：当不同智能体得出矛盾结论时，通过辩论机制逼近真相
- **反思驱动器**：将论坛讨论转化为具体行动，触发智能体的自我修正
- **知识整合器**：将碎片化信息整合为系统化认知

**虚拟会议室架构**：
```
QueryEngine (信息搜集专家) → 提供最新网络动态
MediaEngine (视觉分析专家) → 提供多媒体证据
InsightEngine (数据挖掘专家) → 提供历史模式分析
                      ↓
              ForumEngine (主持人)
                      ↓
          集体智能决策与共识达成
```

#### 3.6.2 主持人决策逻辑与提示词策略

**ForumHost类核心功能**：
```python
class ForumHost:
    """论坛主持人 - 使用Qwen3-235B模型作为智能主持人"""
    
    def generate_host_speech(self, forum_logs: List[str]) -> Optional[str]:
        """
        主持人决策逻辑：
        1. 事件梳理：自动识别关键事件、人物、时间节点
        2. 引导讨论：基于各agent发言，引导深入讨论关键问题
        3. 纠正错误：发现事实错误或逻辑矛盾时明确指出
        4. 整合观点：综合不同视角，形成更全面的认识
        5. 趋势预测：分析舆情发展趋势，提出风险点
        6. 推进分析：提出新的分析角度，引导后续讨论方向
        """
```

**主持人提示词策略**：
```python
def _build_system_prompt(self) -> str:
    """
    主持人系统提示词结构：
    - 角色定义：多agent舆情分析系统的论坛主持人
    - 职责说明：6大核心职责（事件梳理、引导讨论、纠正错误等）
    - Agent介绍：INSIGHT/MEDIA/QUERY三个专业智能体的分工
    - 发言要求：综合性、结构清晰、深入分析、客观中立、前瞻性
    - 注意事项：科研目的、伦理性合规审查、专业性和学术性
    """
```

**主持人引导策略**：
1. **事件梳理与时间线分析**：从各agent发言中自动识别关键事件，按时间顺序整理事件脉络
2. **观点整合与对比分析**：综合三个Agent的视角，指出共识与分歧，分析信息互补性
3. **深层次分析与趋势预测**：基于已有信息分析深层原因，预测舆情发展趋势
4. **问题引导与讨论方向**：提出2-3个关键问题，引导后续研究方向

**发言质量控制**：
- **字数限制**：每次发言控制在1000字以内
- **结构要求**：必须包含四个部分（事件梳理、观点整合、深度分析、问题引导）
- **内容深度**：不仅仅总结已有信息，还要提出深层次见解
- **客观中立**：基于事实进行分析，避免主观臆测

#### 3.6.3 动态调度与协作循环机制

**LogMonitor主循环架构**：
```python
class LogMonitor:
    """基于文件变化的智能日志监控器"""
    
    def monitor_logs(self):
        """
        异步监控循环机制：
        1. 实时监控三个引擎的日志文件变化
        2. 智能识别SummaryNode输出（首次总结和反思总结）
        3. 多行JSON内容捕获与解析
        4. ERROR块过滤机制，确保数据质量
        5. 主持人发言触发机制（每5条agent发言触发一次）
        """
```

**协作循环时序控制**：
```
Agent发言 → 日志监控 → 内容提取 → 主持人评估 → 主持人发言
    ↓
Agent接收主持人建议 → 自我修正 → 新一轮搜索 → 新发言
```

**异步调度策略**：
- **非阻塞监控**：主循环不阻塞其他引擎的正常执行
- **增量处理**：只处理新增的日志内容，避免重复处理
- **阈值触发**：每5条agent发言触发一次主持人发言
- **状态管理**：维护文件位置、JSON捕获状态、ERROR块状态

**智能内容识别算法**：
```python
def is_target_log_line(self, line: str) -> bool:
    """
    目标日志行识别策略：
    1. 类名识别：FirstSummaryNode、ReflectionSummaryNode
    2. 模块路径识别：InsightEngine.nodes.summary_node等
    3. 关键标识文本："正在生成首次段落总结"、"正在生成反思总结"
    4. 排除条件：ERROR级别日志、错误关键词、短小提示信息
    """
```

#### 3.6.4 冲突解决与共识机制

**冲突检测算法**：
```python
def detect_conflicts(self, agent_speeches: List[Dict]) -> List[Conflict]:
    """
    冲突检测策略：
    1. 事实性冲突：同一事件的不同描述
    2. 情感倾向冲突：正面vs负面评价
    3. 数据矛盾：不同来源的统计数据差异
    4. 时间线冲突：事件发生时间的不一致
    """
```

**典型冲突场景处理**：

**场景1：QueryEngine vs InsightEngine 数据矛盾**
```
QueryEngine：发现大量正面新闻报道（媒体视角）
InsightEngine：发现社交媒体负面评论占主导（用户视角）
主持人策略：
- 指出媒体与用户视角的差异是正常现象
- 引导分析"为什么媒体正面但用户负面"
- 建议QueryEngine搜索"媒体公关策略"相关报道
- 建议InsightEngine分析负面评论的具体原因
```

**场景2：MediaEngine vs QueryEngine 时间线冲突**
```
MediaEngine：图片显示事件发生在A时间
QueryEngine：新闻报道显示事件发生在B时间
主持人策略：
- 指出可能存在信息传播延迟
- 引导验证原始信息来源的可靠性
- 建议交叉验证多个独立信息源
- 提出"信息传播路径分析"的研究方向
```

**共识达成机制**：
1. **多轮辩论**：通过主持人引导进行多轮观点交换
2. **证据权重**：对不同来源的证据进行可信度评估
3. **妥协方案**：当无法达成完全一致时，提出妥协性结论
4. **不确定性标注**：明确标注仍存在争议的部分

#### 3.6.5 反思驱动机制与上下文注入

**从言语到行动的转化机制**：
```python
# 在QueryEngine的反思节点中注入主持人建议
class ReflectionNode:
    def run(self, state, host_speech: str):
        """
        反思驱动流程：
        1. 读取最新主持人发言
        2. 解析主持人建议的具体行动点
        3. 将建议转化为具体的搜索策略调整
        4. 执行新一轮的深度搜索
        5. 生成基于主持人指导的反思总结
        """
```

**上下文压缩与注入技术**：
```python
def format_host_speech_for_prompt(host_speech: str) -> str:
    """
    主持人发言格式化策略：
    1. 提取核心观点和建议，去除冗余描述
    2. 结构化组织：事件梳理→观点整合→深度分析→问题引导
    3. 保持关键数据点和具体建议的完整性
    4. 控制长度，确保能有效注入到Agent的Prompt中
    """
    
    return f"""
### 论坛主持人最新总结
以下是论坛主持人对各Agent讨论的最新总结和引导：

{host_speech}

---
"""
```

**反思驱动的具体行动转化**：

**主持人建议**："建议QueryAgent关注事件的时间线一致性，验证不同来源的时间信息"

**转化为具体行动**：
```python
# QueryEngine的反思搜索调整
reflection_search_strategy = {
    "搜索关键词": ["事件时间线", "时间一致性验证", "信息传播延迟"],
    "时间范围": "自定义日期范围搜索",
    "信息来源": "权威媒体和官方声明",
    "验证重点": "交叉验证多个独立信息源"
}
```

**自我修正机制**：
- **认知偏差修正**：通过主持人引导发现自身分析的局限性
- **信息盲点补充**：基于其他Agent的发现补充自身的信息盲点
- **分析方法优化**：借鉴其他Agent的分析方法和视角
- **结论重新评估**：在集体讨论的基础上重新评估原有结论

#### 3.6.6 数据结构与状态管理

**论坛日志数据结构**：
```python
# 论坛日志条目格式
forum_log_entry = {
    "timestamp": "14:30:25",           # 时间戳
    "speaker": "INSIGHT",              # 发言者：INSIGHT/MEDIA/QUERY/HOST/SYSTEM
    "content": "分析内容..."           # 发言内容（支持多行）
}
```

**日志文件格式规范**：
```
[14:30:25] [INSIGHT] 基于舆情数据库分析，发现用户对品牌A的负面评论主要集中在...
[14:31:10] [MEDIA] 图片分析显示，品牌A的广告投放主要集中在...
[14:32:05] [QUERY] 最新新闻报道表明，品牌A近期发布了重大产品更新...
[14:33:20] [HOST] 综合各位的分析，我发现一个关键矛盾：用户负面评论与媒体正面报道...
```

**状态管理机制**：
```python
class LogMonitor:
    """状态管理关键字段"""
    
    def __init__(self):
        # 文件位置跟踪
        self.file_positions = {}           # 每个文件的读取位置
        self.file_line_counts = {}         # 每个文件的行数基线
        
        # JSON捕获状态
        self.capturing_json = {}           # 是否正在捕获JSON
        self.json_buffer = {}              # JSON缓冲区
        self.json_start_line = {}          # JSON开始行
        
        # ERROR块过滤
        self.in_error_block = {}           # 是否在ERROR块中
        
        # 主持人发言管理
        self.agent_speeches_buffer = []    # Agent发言缓冲区
        self.host_speech_threshold = 5     # 触发阈值
        self.is_host_generating = False    # 生成状态标志
```

**数据质量保障机制**：
1. **ERROR块过滤**：遇到ERROR级别日志时自动跳过后续内容
2. **JSON解析鲁棒性**：支持多行JSON捕获和语法错误修复
3. **内容去重**：避免重复记录相同的发言内容
4. **格式标准化**：统一时间戳格式和发言者标签

#### 3.6.7 智能体响应与集成机制

**QueryEngine的论坛响应集成**：
```python
class DeepSearchAgent:
    def research(self, query: str):
        """
        研究流程中的论坛集成：
        1. 生成报告结构后，检查是否有主持人建议
        2. 处理每个段落时，注入主持人指导
        3. 反思阶段基于论坛讨论调整搜索策略
        4. 最终报告整合论坛讨论的集体智慧
        """
        
        # 在反思阶段注入主持人建议
        host_speech = get_latest_host_speech()
        if host_speech:
            self._inject_host_guidance(host_speech)
```

**智能体间的信息互补**：

**信息流整合**：
```
QueryEngine → 最新网络动态 + 权威媒体报道
MediaEngine → 视觉证据 + 多媒体传播效果
InsightEngine → 历史数据模式 + 用户情感分析
                    ↓
            ForumEngine整合 → 多维度的完整认知
```

**协同分析优势**：
- **时效性互补**：QueryEngine提供最新信息，InsightEngine提供历史背景
- **视角互补**：MediaEngine提供视觉证据，其他引擎提供文本分析
- **数据源互补**：公开网络信息 + 私有数据库 + 多媒体内容
- **分析方法互补**：搜索技术 + 情感分析 + 视觉识别

#### 3.6.8 性能优化与容错机制

**监控性能优化**：
- **增量读取**：只读取文件新增部分，避免全量扫描
- **智能休眠**：当没有新内容时自动延长检查间隔
- **内存管理**：定期清理缓冲区，防止内存泄漏
- **并发控制**：使用锁机制确保线程安全的文件写入

**容错与恢复机制**：
```python
def read_new_lines(self, file_path: Path, app_name: str) -> List[str]:
    """
    容错读取策略：
    1. 文件大小异常检测：如果文件变小，重新从头开始
    2. 编码错误处理：使用errors='ignore'参数
    3. 权限异常处理：捕获文件访问权限异常
    4. 状态重置：在异常情况下重置捕获状态
    """
```

**主持人发言失败处理**：
- **超时控制**：设置合理的API调用超时时间
- **降级策略**：当主持人不可用时，使用简单的总结机制
- **重试机制**：支持有限次数的重试尝试
- **错误隔离**：主持人失败不影响其他引擎的正常运行

**系统稳定性保障**：
- **资源隔离**：论坛监控与其他引擎的资源使用隔离
- **异常传播控制**：局部异常不会导致整个系统崩溃
- **状态持久化**：关键状态可以持久化到磁盘
- **监控告警**：关键异常情况的实时告警机制

## 4. 数据需求
### 4.1 核心数据实体关系
#### 4.1.1 用户数据模型
#### 4.1.2 内容数据模型
#### 4.1.3 评论数据模型
#### 4.1.4 情感分析数据模型
#### 4.1.5 报告数据模型

### 4.2 数据库设计规范
#### 4.2.1 表结构设计原则
#### 4.2.2 索引优化策略
#### 4.2.3 数据分区方案
#### 4.2.4 备份恢复机制

### 4.3 数据存储与处理
#### 4.3.1 实时数据处理
#### 4.3.2 批量数据处理
#### 4.3.3 数据清洗规范
#### 4.3.4 数据质量监控

## 5. 非功能性需求
### 5.1 性能需求
#### 5.1.1 响应时间要求
#### 5.1.2 并发处理能力
#### 5.1.3 数据吞吐量
#### 5.1.4 系统可扩展性

### 5.2 安全需求
#### 5.2.1 数据安全保护
#### 5.2.2 API访问控制
#### 5.2.3 用户隐私保护
#### 5.2.4 防爬虫机制

### 5.3 可靠性需求
#### 5.3.1 系统可用性
#### 5.3.2 容错处理机制
#### 5.3.3 数据一致性
#### 5.3.4 故障恢复能力

### 5.4 部署与运维需求
#### 5.4.1 容器化部署方案
#### 5.4.2 监控告警体系
#### 5.4.3 日志管理规范
#### 5.4.4 版本升级策略

## 6. 接口与交互
### 6.1 API接口设计
#### 6.1.1 RESTful API规范
#### 6.1.2 WebSocket实时通信
#### 6.1.3 文件上传下载接口
#### 6.1.4 错误码定义规范

### 6.2 用户交互流程
#### 6.2.1 分析任务创建流程
#### 6.2.2 实时进度监控
#### 6.2.3 报告查看与导出
#### 6.2.4 系统配置管理

### 6.3 外部系统集成
#### 6.3.1 LLM模型API集成
#### 6.3.2 第三方数据源接入
#### 6.3.3 企业系统对接
#### 6.3.4 消息通知机制

---

**说明**：此框架大纲基于对BettaFish项目的逆向工程分析，涵盖了项目的核心架构、功能模块和技术栈。每个章节都对应项目的实际功能模块，为后续详细需求规格说明提供了完整的结构基础。